# -*- coding: utf-8 -*-
"""CS230.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rXhtCWaU7BlI6HG-Gi8HFFXHTWfysGcY

Import Dependencies
"""

oimport sys
import os
import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split
import random as rn
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from numpy import array, savetxt, asarray, save
from keras_preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Dropout
from keras.utils import np_utils
from google.colab import drive
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from tensorflow import keras
drive.mount('/content/drive')

"""Filter Dataset Into Smaller Segments"""

new = open("/content/drive/MyDrive/CS230/processed_data.txt", "w")
for line in open("/content/drive/MyDrive/CS230/data.txt", "r"):  
        if 30 < len(line) < 50:
            new.write(line)
file_new = np.array(list(open('/content/drive/MyDrive/CS230/processed_data.txt')))
print(file_new.shape)

"""Find Maximum Sequence Length"""

file = open("/content/drive/MyDrive/CS230/processed_data.txt")
max_seq_len = int(len(max(file,key=len)))
print ("Max Sequence Length: ", max_seq_len)

"""Define Functions"""

def read(fileName):
        fileObj = open(fileName, "r")
        words = fileObj.read().splitlines()
        fileObj.close()
        return words

def padFile(fileName):
  temp = read(fileName)
  preprocessed_pad_text = [['?'] + list(i) for i in temp]
  print("Sample 1: ", preprocessed_pad_text[0])
  print("Sample 2: ", preprocessed_pad_text[1])
  print("Sample 3: ", preprocessed_pad_text[2])
  print("Sample 4: ", preprocessed_pad_text[3])
  print("Sample 5: ", preprocessed_pad_text[4])

  padded_text = pad_sequences(preprocessed_pad_text, dtype=object, maxlen=max_seq_len+1, padding="post", value="!")
  # front_pad = [['?'] + i for i in padded_text]
  print("Padded Text Arrays: ", padded_text)

  var = ["".join(i) for i in padded_text]
  print("Padded Strings: ", var[0:5])
  print(var)
  # with open('/content/drive/MyDrive/CS230/padded_processed_data.txt', "w") as output:
  #   for x in var:
  #     output.write(y)
  # np.array(var)
  # np.savetxt('/content/drive/MyDrive/CS230/padded_processed_data.txt', var, fmt ='%s', newline='')
  return var

var = padFile('/content/drive/MyDrive/CS230/processed_data.txt')
var = np.array(var)

"""Load & Save Datasets"""

#init random seed
seed = 1
np.random.seed(seed)
#split data into train/test
full_train, test = train_test_split(np.array(var), test_size=0.25, random_state=seed)
# full_train, test = train_test_split(np.array(var), test_size=0.25, random_state=seed)

#split full train set into smaller train set and validation (dev) set
# np.savetxt('/content/drive/MyDrive/CS230/full_train_data.txt', full_train, fmt ='%s', newline='')
train, val = train_test_split(np.array(full_train), test_size=0.10, random_state=seed)
np.savetxt('/content/drive/MyDrive/CS230/train_data.txt', train, fmt ='%s', newline='')
np.savetxt('/content/drive/MyDrive/CS230/test_data.txt', test, fmt ='%s', newline='')
np.savetxt('/content/drive/MyDrive/CS230/val_data.txt', val, fmt='%s', newline='')
print("Sample:", train[seed])
print("Train:", train.shape)
print("Validation:", val.shape)
print("Test:", test.shape)

"""Data Concatenation"""

train = pd.read_fwf('/content/drive/MyDrive/CS230/train_data.txt')

def concatenate(l):
    res = ''
    for word in train:
      # print(word)
      res += word
    return res

train = concatenate(train)

print(train[0:100])

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None, char_level=True, lower=False)
tokenizer.fit_on_texts(train)

new_train = tokenizer.texts_to_sequences(train)

print(new_train[0:100])
print(train[0:100])

"""Data Breakdown"""

n_chars = len(train)
n_vocab = len(list(set(train)))
print(n_chars)
print(n_vocab)

n_chars = len(new_train)
n_vocab = len(set(train))
print(n_chars)
print(n_vocab)

"""N-Grams Sequence"""

seqLen = 15
stepSize = 3
input_chars = []
next_char = []

# for example in train:
for i in range(0, len(new_train) - seqLen, stepSize):
  input_chars.append(new_train[i : i + seqLen])
  next_char.append(new_train[i + seqLen])

# input = []
# for x in input_chars:
#   temp = []
#   for y in x:
#     temp.append(y[0])
#   input.append(temp)

# print(train[0])
for i in range(5):
  print(input_chars[i])
  print(next_char[i])

x_train = np.array(input_chars)
x_train.flatten()
y_train = np.array(next_char)
y_train_2 = np_utils.to_categorical(y_train)

"""Model Definition"""

baselineLSTM = Sequential()
baselineLSTM.add(LSTM(units = 64, input_shape = (x_train.shape[1:])))
# baselineLSTM.add(LSTM(units = 64, return_sequences= True, input_shape = (x_train.shape[1:])))
# baselineLSTM.add(Dropout(0.2))
# baselineLSTM.add(LSTM(units = 128))
baselineLSTM.add(Dropout(0.2))
baselineLSTM.add(Dense(units = 48, activation='softmax'))
baselineLSTM.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

print(baselineLSTM.summary())

"""Train LSTM Model"""

baselineLSTM.fit(x_train, y_train_2, epochs = 1, batch_size = 512)

#INSERT PATH HERE TO SAVE MODEL TO
path = ''

baselineLSTM.save(path)