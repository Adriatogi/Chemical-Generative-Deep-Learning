{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","import pandas as pd\n","import numpy as np\n","import json\n","from sklearn.model_selection import train_test_split\n","import random as rn\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from numpy import array, savetxt, asarray, save\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Dropout\n","from keras.utils import np_utils\n","#from google.colab import drive\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras.models import load_model\n","from keras.layers import GRU\n","#drive.mount('/content/drive')\n","import keras_tuner\n","from keras_tuner import HyperModel\n","\n","DATA_PATH = \"data/data.txt\"\n","NEW_DATA = \"data/processed_data.txt\"\n","TRAIN_DATA = \"data/train_data.txt\"\n","TEST_DATA = \"data/test_data.txt\"\n","VAL_DATA = \"data/val_data.txt\""]},{"cell_type":"markdown","metadata":{},"source":["Filter Dataset Into Smaller Segments"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(123653,)\n"]}],"source":["count=0\n","new = open(NEW_DATA, \"w\")\n","for line in open(DATA_PATH, \"r\"):\n","    count += 1\n","    if 30 < len(line) < 50 and count < 300000:\n","        if ('T' not in line) and ('V' not in line) and ('g' not in line) and ('L' not in line) and ('8' not in line):\n","            new.write(line)\n","file_new = np.array(list(open(NEW_DATA)))\n","print(file_new.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["Find Maximum Sequence Length"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Max Sequence Length:  49\n"]}],"source":["file = open(NEW_DATA)\n","max_seq_len = int(len(max(file,key=len)))\n","print (\"Max Sequence Length: \", max_seq_len)"]},{"cell_type":"markdown","metadata":{},"source":["Define Functions"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample 1:  ['?', 'C', 'O', 'c', '1', 'c', 'c', '2', 'n', 'c', '(', 'n', 'c', '(', 'N', ')', 'c', '2', 'c', 'c', '1', 'O', 'C', ')', 'N', '3', 'C', 'C', 'N', '(', 'C', 'C', '3', ')', 'C', '(', '=', 'O', ')', 'c', '4', 'o', 'c', 'c', 'c', '4']\n","Sample 2:  ['?', 'C', 'C', 'N', '1', 'C', '=', 'C', '(', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'c', '2', 'c', 'c', 'c', '(', 'C', ')', 'n', 'c', '1', '2']\n","Sample 3:  ['?', 'C', 'C', 'N', '1', 'C', '=', 'C', '(', 'C', '(', '=', 'O', ')', 'O', ')', 'C', '(', '=', 'O', ')', 'c', '2', 'c', 'c', '(', 'F', ')', 'c', '(', 'c', 'c', '1', '2', ')', 'N', '3', 'C', 'C', 'N', 'C', 'C', '3']\n","Sample 4:  ['?', 'C', 'N', '1', 'C', '(', '=', 'O', ')', 'C', 'N', '=', 'C', '(', 'c', '2', 'c', 'c', 'c', 'c', 'c', '2', ')', 'c', '3', 'c', 'c', '(', 'C', 'l', ')', 'c', 'c', 'c', '1', '3']\n","Sample 5:  ['?', 'O', 'C', '1', '=', 'N', 'C', '(', '=', 'O', ')', 'C', '(', 'N', '1', ')', '(', 'c', '2', 'c', 'c', 'c', 'c', 'c', '2', ')', 'c', '3', 'c', 'c', 'c', 'c', 'c', '3']\n","Padded Strings:  ['?COc1cc2nc(nc(N)c2cc1OC)N3CCN(CC3)C(=O)c4occc4!!!!', '?CCN1C=C(C(=O)O)C(=O)c2ccc(C)nc12!!!!!!!!!!!!!!!!!', '?CCN1C=C(C(=O)O)C(=O)c2cc(F)c(cc12)N3CCNCC3!!!!!!!', '?CN1C(=O)CN=C(c2ccccc2)c3cc(Cl)ccc13!!!!!!!!!!!!!!', '?OC1=NC(=O)C(N1)(c2ccccc2)c3ccccc3!!!!!!!!!!!!!!!!']\n"]}],"source":["\n","def read(fileName): \n","        fileObj = open(fileName, \"r\") \n","        words = fileObj.read().splitlines() \n","        fileObj.close() \n","        return words \n","def padFile(fileName): \n","  temp = read(fileName) \n","  preprocessed_pad_text = [['?'] + list(i) for i in temp] \n","  print(\"Sample 1: \", preprocessed_pad_text[0]) \n","  print(\"Sample 2: \", preprocessed_pad_text[1]) \n","  print(\"Sample 3: \", preprocessed_pad_text[2]) \n","  print(\"Sample 4: \", preprocessed_pad_text[3]) \n","  print(\"Sample 5: \", preprocessed_pad_text[4]) \n","  padded_text = pad_sequences(preprocessed_pad_text, dtype=object, maxlen=max_seq_len+1, padding=\"post\", value=\"!\") \n","  # front_pad = [['?'] + i for i in padded_text] \n","  #print(\"Padded Text Arrays: \", padded_text) \n","  var = [\"\".join(i) for i in padded_text] \n","  print(\"Padded Strings: \", var[0:5]) \n","  #print(var) \n","  # with open('/content/drive/MyDrive/CS230/padded_processed_data.txt', \"w\") as output: \n","  #   for x in var: \n","  #     output.write(y) \n","  # np.array(var) \n","  # np.savetxt('/content/drive/MyDrive/CS230/padded_processed_data.txt', var, fmt ='%s', newline='') \n","  return var \n","var = padFile(NEW_DATA) \n","var = np.array(var) \n"]},{"cell_type":"markdown","metadata":{},"source":["Load & Save Datasets"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#init random seed\n","seed = 1\n","np.random.seed(seed)\n","#split data into train/test\n","full_train, test = train_test_split(np.array(var), test_size=0.2, random_state=seed)\n","# full_train, test = train_test_split(np.array(var), test_size=0.25, random_state=seed)"]},{"cell_type":"markdown","metadata":{},"source":["split full train set into smaller train set and validation (dev) set"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample: ?COc1cccc(CNCCCCCCNCCSSCCNCCCCCCNCc2ccccc2OC)c1!!!\n","Train: (89029,)\n","Validation: (9893,)\n","Test: (24731,)\n"]}],"source":["#np.savetxt('/content/drive/MyDrive/CS230/full_train_data.txt', full_train, fmt ='%s', newline='')\n","train, val = train_test_split(np.array(full_train), test_size=0.10, random_state=seed)\n","np.savetxt(TRAIN_DATA, train, fmt ='%s', newline='')\n","np.savetxt(TEST_DATA, test, fmt ='%s', newline='')\n","np.savetxt(VAL_DATA, val, fmt='%s', newline='')\n","print(\"Sample:\", train[seed])\n","print(\"Train:\", train.shape)\n","print(\"Validation:\", val.shape)\n","print(\"Test:\", test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Train Data Processing"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["89029\n","?N[C@@H](Cc1c[nH]c2ccccc12)C(=O)OCc3ccccc3!!!!!!!!?COc1cccc(CNCCCCCCNCCSSCCNCCCCCCNCc2ccccc2OC)c1!!!\n"]}],"source":["#Load Data (optional: only if previous cells are not run and data is saved already) \n","# train = pd.read_fwf('/content/drive/MyDrive/CS230/train_data.txt') \n","#Concatenate Data \n","def concatenate(data): \n","    #res = '' \n","    print(len(data)) \n","    #for count, word in enumerate(data): \n","        #if count %1000 == 0: \n","            #print(count) \n","            #pass \n","        # print(word) \n","        #res += word \n","    res = ''.join(data) \n","    return res \n","train = concatenate(train) \n","print(train[0:100]) \n","\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[12], [9], [15], [3], [17], [17], [18], [16], [4], [3], [1], [7], [1], [15], [13], [18], [16], [1], [8], [1], [1], [1], [1], [1], [7], [8], [5], [3], [4], [10], [6], [5], [6], [3], [1], [11], [1], [1], [1], [1], [1], [11], [2], [2], [2], [2], [2], [2], [2], [2], [12], [3], [6], [1], [7], [1], [1], [1], [1], [4], [3], [9], [3], [3], [3], [3], [3], [3], [9], [3], [3], [22], [22], [3], [3], [9], [3], [3], [3], [3], [3], [3], [9], [3], [1], [8], [1], [1], [1], [1], [1], [8], [6], [3], [5], [1], [7], [2], [2], [2]]\n","?N[C@@H](Cc1c[nH]c2ccccc12)C(=O)OCc3ccccc3!!!!!!!!?COc1cccc(CNCCCCCCNCCSSCCNCCCCCCNCc2ccccc2OC)c1!!!\n"]}],"source":["#Tokenize Data \n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None, char_level=True, lower=False) \n","tokenizer.fit_on_texts(train) \n","new_train = tokenizer.texts_to_sequences(train) \n","print(new_train[0:100]) \n","print(train[0:100]) "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# of Unique Characters: 4451450\n","# of Total Characters: 42\n","# of Unique Characters: 4451450\n","# of Total Characters: 42\n","Input Sequence: [[12], [9], [15], [3], [17], [17], [18], [16], [4], [3], [1], [7], [1], [15], [13]]\n","Next Character Prediction: [18]\n","Input Sequence: [[9], [15], [3], [17], [17], [18], [16], [4], [3], [1], [7], [1], [15], [13], [18]]\n","Next Character Prediction: [16]\n","Input Sequence: [[15], [3], [17], [17], [18], [16], [4], [3], [1], [7], [1], [15], [13], [18], [16]]\n","Next Character Prediction: [1]\n","Input Sequence: [[3], [17], [17], [18], [16], [4], [3], [1], [7], [1], [15], [13], [18], [16], [1]]\n","Next Character Prediction: [8]\n","Input Sequence: [[17], [17], [18], [16], [4], [3], [1], [7], [1], [15], [13], [18], [16], [1], [8]]\n","Next Character Prediction: [1]\n","(4451435, 15, 1)\n","(4451435, 1)\n","[[[12]\n","  [ 9]\n","  [15]\n","  [ 3]\n","  [17]\n","  [17]\n","  [18]\n","  [16]\n","  [ 4]\n","  [ 3]\n","  [ 1]\n","  [ 7]\n","  [ 1]\n","  [15]\n","  [13]]\n","\n"," [[ 9]\n","  [15]\n","  [ 3]\n","  [17]\n","  [17]\n","  [18]\n","  [16]\n","  [ 4]\n","  [ 3]\n","  [ 1]\n","  [ 7]\n","  [ 1]\n","  [15]\n","  [13]\n","  [18]]\n","\n"," [[15]\n","  [ 3]\n","  [17]\n","  [17]\n","  [18]\n","  [16]\n","  [ 4]\n","  [ 3]\n","  [ 1]\n","  [ 7]\n","  [ 1]\n","  [15]\n","  [13]\n","  [18]\n","  [16]]\n","\n"," [[ 3]\n","  [17]\n","  [17]\n","  [18]\n","  [16]\n","  [ 4]\n","  [ 3]\n","  [ 1]\n","  [ 7]\n","  [ 1]\n","  [15]\n","  [13]\n","  [18]\n","  [16]\n","  [ 1]]\n","\n"," [[17]\n","  [17]\n","  [18]\n","  [16]\n","  [ 4]\n","  [ 3]\n","  [ 1]\n","  [ 7]\n","  [ 1]\n","  [15]\n","  [13]\n","  [18]\n","  [16]\n","  [ 1]\n","  [ 8]]]\n","[[18]\n"," [16]\n"," [ 1]\n"," [ 8]\n"," [ 1]]\n"]}],"source":["#Print Data Breakdown \n","n_chars = len(train) \n","n_vocab = len(list(set(train))) \n","print(\"# of Unique Characters:\", n_chars) \n","print(\"# of Total Characters:\", n_vocab) \n","n_chars = len(new_train) \n","n_vocab = len(set(train)) \n","print(\"# of Unique Characters:\", n_chars) \n","print(\"# of Total Characters:\", n_vocab) \n","#N-Grams Sequence \n","seqLen = 15 \n","stepSize = 1 \n","input_chars = [] \n","next_char = [] \n","for i in range(0, len(new_train) - seqLen, stepSize): \n","  input_chars.append(new_train[i : i + seqLen]) \n","  next_char.append(new_train[i + seqLen]) \n","for i in range(5): \n","  print(\"Input Sequence:\", input_chars[i]) \n","  print(\"Next Character Prediction:\", next_char[i]) \n","#Assemble Train Datasets \n","x_train = np.array(input_chars) \n","x_train.flatten() \n","y_train = np.array(next_char) \n","y_train_2 = np_utils.to_categorical(y_train) \n","print(x_train.shape) \n","print(y_train.shape) \n","print(x_train[0:5]) \n","print(y_train[0:5]) "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def generate_text(seed_text, next_words, model, max_sequence_len):\n","    for _ in range(next_words):\n","        ind1 = 0\n","        ind2 = 14\n","        temp = []\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = np.array(token_list)\n","        for token in token_list:\n","            temp.append(token)\n","        \n","        #token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","        token_list=np.array([temp])\n","        token_list = token_list[ind1:ind2+1]\n","        print(token_list.shape)\n","        ind1 += 1\n","        \n","        ind2 += 1\n","        print(token_list)\n","        #np.reshape(token_list, (15,1))\n","        predicted = model.predict(token_list, verbose=0)\n","        #print(predicted)\n","        ind=np.argmax(predicted)\n","        print(ind)\n","        print(f\"Shape: {predicted.shape}\")\n","        output_word = \"\"\n","        for word,index in tokenizer.word_index.items():\n","            '''\n","            print(tokenizer.word_index.items())\n","            print(word)\n","            print(ind)\n","            print(index)\n","            '''\n","            \n","            if index == ind:\n","                output_word = word\n","                break\n","        seed_text += output_word\n","    return seed_text.title()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 15, 32)            4352      \n","                                                                 \n"," dropout (Dropout)           (None, 15, 32)            0         \n","                                                                 \n"," lstm_1 (LSTM)               (None, 64)                24832     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense (Dense)               (None, 43)                2795      \n","                                                                 \n","=================================================================\n","Total params: 31,979\n","Trainable params: 31,979\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","(1, 15)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3]]\n","4\n","Shape: (1, 43)\n","(1, 16)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4]]\n","10\n","Shape: (1, 43)\n","(1, 17)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10]]\n","6\n","Shape: (1, 43)\n","(1, 18)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6]]\n","5\n","Shape: (1, 43)\n","(1, 19)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5]]\n","9\n","Shape: (1, 43)\n","(1, 20)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9]]\n","3\n","Shape: (1, 43)\n","(1, 21)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3]]\n","4\n","Shape: (1, 43)\n","(1, 22)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4]]\n","10\n","Shape: (1, 43)\n","(1, 23)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10]]\n","6\n","Shape: (1, 43)\n","(1, 24)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6]]\n","5\n","Shape: (1, 43)\n","(1, 25)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5]]\n","1\n","Shape: (1, 43)\n","(1, 26)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1]]\n","8\n","Shape: (1, 43)\n","(1, 27)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8]]\n","1\n","Shape: (1, 43)\n","(1, 28)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1]]\n","1\n","Shape: (1, 43)\n","(1, 29)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1]]\n","1\n","Shape: (1, 43)\n","(1, 30)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1]]\n","1\n","Shape: (1, 43)\n","(1, 31)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1]]\n","1\n","Shape: (1, 43)\n","(1, 32)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1]]\n","8\n","Shape: (1, 43)\n","(1, 33)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8]]\n","3\n","Shape: (1, 43)\n","(1, 34)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3]]\n","4\n","Shape: (1, 43)\n","(1, 35)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4]]\n","10\n","Shape: (1, 43)\n","(1, 36)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10]]\n","6\n","Shape: (1, 43)\n","(1, 37)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6]]\n","5\n","Shape: (1, 43)\n","(1, 38)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5]]\n","9\n","Shape: (1, 43)\n","(1, 39)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9]]\n","3\n","Shape: (1, 43)\n","(1, 40)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3]]\n","3\n","Shape: (1, 43)\n","(1, 41)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 42)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 43)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 44)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 45)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3  3  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 46)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3  3  3  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 47)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3  3  3  3  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 48)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3  3  3  3  3  3  3]]\n","3\n","Shape: (1, 43)\n","(1, 49)\n","[[12  3  9  4  3  5  3  7 10  9  3  4 10  9  3  4 10  6  5  9  3  4 10  6\n","   5  1  8  1  1  1  1  1  8  3  4 10  6  5  9  3  3  3  3  3  3  3  3  3\n","   3]]\n","3\n","Shape: (1, 43)\n","?Cn(C)C1=Nc(=Nc(=O)Nc(=O)C2Ccccc2C(=O)Nccccccccccc\n","1/1 [==============================] - 0s 28ms/step\n","106\n","dict_items([('c', 1), ('!', 2), ('C', 3), ('(', 4), (')', 5), ('O', 6), ('1', 7), ('2', 8), ('N', 9), ('=', 10), ('3', 11), ('?', 12), ('n', 13), ('4', 14), ('[', 15), (']', 16), ('@', 17), ('H', 18), ('\\\\', 19), ('l', 20), ('F', 21), ('S', 22), ('5', 23), ('s', 24), ('/', 25), ('#', 26), ('o', 27), ('+', 28), ('B', 29), ('r', 30), ('-', 31), ('P', 32), ('.', 33), ('I', 34), ('6', 35), ('a', 36), ('e', 37), ('i', 38), ('7', 39), ('p', 40), ('K', 41), ('A', 42)])\n"]}],"source":["#seed ='!!!!!!!!!!!!!!?'\n","seed = '?CN(C)C1=NC(=NC'\n","model = keras.models.load_model('baseline_lstm.h5')\n","print(model.summary())\n","result = generate_text(seed, 35, model, 50)\n","print(result)\n","test = model.predict([[3], [20], [1], [7], [1], [1], [1], [1], [8], [1], [1], [4], [3], [3], [11]])\n","test = np.argmax(test)\n","print(test)\n","print(tokenizer.word_index.items())"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["#Load Data (optional: only if previous cells are not run and data is saved already) \n","#val = pd.read_fwf('/content/drive/MyDrive/CS230/val_data.txt')"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["' \\n#Concatenate Data \\ndef concatenate(data): \\n    #res = \\'\\' \\n    #for count, word in enumerate(data): \\n     #   if count %100 == 0: \\n      #      print(count) \\n     \\n      # print(word) \\n      #  res += word \\n    res = \\'\\'.join(data) \\n    return res \\nval = concatenate(val) \\nprint(val[0:100]) \\n#Tokenize Data \\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None, char_level=True, lower=False) \\ntokenizer.fit_on_texts(val) \\nnew_val = tokenizer.texts_to_sequences(val) \\nprint(new_val[0:100]) \\nprint(val[0:100]) \\n#Print Data Breakdown \\nn_chars = len(val) \\nn_vocab = len(list(set(val))) \\nprint(\"# of Unique Characters:\", n_chars) \\nprint(\"# of Total Characters:\", n_vocab) \\nn_chars = len(new_val) \\nn_vocab = len(set(val)) \\nprint(\"# of Unique Characters:\", n_chars) \\nprint(\"# of Total Characters:\", n_vocab) \\nprint(\"Difference\", set(train).difference(set(val))) \\n#N-Grams Sequence \\nseqLen = 15 \\nstepSize = 1 \\ninput_chars = [] \\nnext_char = [] \\nfor i in range(0, len(new_val) - seqLen, stepSize): \\n  input_chars.append(new_val[i : i + seqLen]) \\n  next_char.append(new_val[i + seqLen]) \\n#print(set(next_char.items)) \\nfor i in range(5): \\n  print(\"Input Sequence:\", input_chars[i]) \\n  print(\"Next Character Prediction:\", next_char[i]) \\n#Assemble Validation Datasets \\nx_val = np.array(input_chars) \\nx_val.flatten() \\ny_val = np.array(next_char) \\ny_val_2 = np_utils.to_categorical(y_val) \\nprint(y_val_2) \\nprint(y_train_2) \\n#score = model.evaluate(x_val, y_val_2) \\n#print(score[1]) \\n#print(result) \\n'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["\n","''' \n","#Concatenate Data \n","def concatenate(data): \n","    #res = '' \n","    #for count, word in enumerate(data): \n","     #   if count %100 == 0: \n","      #      print(count) \n","     \n","      # print(word) \n","      #  res += word \n","    res = ''.join(data) \n","    return res \n","val = concatenate(val) \n","print(val[0:100]) \n","#Tokenize Data \n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None, char_level=True, lower=False) \n","tokenizer.fit_on_texts(val) \n","new_val = tokenizer.texts_to_sequences(val) \n","print(new_val[0:100]) \n","print(val[0:100]) \n","#Print Data Breakdown \n","n_chars = len(val) \n","n_vocab = len(list(set(val))) \n","print(\"# of Unique Characters:\", n_chars) \n","print(\"# of Total Characters:\", n_vocab) \n","n_chars = len(new_val) \n","n_vocab = len(set(val)) \n","print(\"# of Unique Characters:\", n_chars) \n","print(\"# of Total Characters:\", n_vocab) \n","print(\"Difference\", set(train).difference(set(val))) \n","#N-Grams Sequence \n","seqLen = 15 \n","stepSize = 1 \n","input_chars = [] \n","next_char = [] \n","for i in range(0, len(new_val) - seqLen, stepSize): \n","  input_chars.append(new_val[i : i + seqLen]) \n","  next_char.append(new_val[i + seqLen]) \n","#print(set(next_char.items)) \n","for i in range(5): \n","  print(\"Input Sequence:\", input_chars[i]) \n","  print(\"Next Character Prediction:\", next_char[i]) \n","#Assemble Validation Datasets \n","x_val = np.array(input_chars) \n","x_val.flatten() \n","y_val = np.array(next_char) \n","y_val_2 = np_utils.to_categorical(y_val) \n","print(y_val_2) \n","print(y_train_2) \n","#score = model.evaluate(x_val, y_val_2) \n","#print(score[1]) \n","#print(result) \n","'''\n"]},{"cell_type":"markdown","metadata":{},"source":["Model Definition"]},{"cell_type":"markdown","metadata":{},"source":["\n"," \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["baselineLSTM = Sequential()\n","# baselineLSTM.add(LSTM(units = 64, input_shape = (x_train.shape[1:])))\n","baselineLSTM.add(LSTM(units = 32, return_sequences= True, input_shape = (x_train.shape[1:])))\n","baselineLSTM.add(Dropout(0.2))\n","baselineLSTM.add(LSTM(units = 64))\n","baselineLSTM.add(Dropout(0.2))\n","baselineLSTM.add(Dense(units = 43, activation='softmax'))\n","baselineLSTM.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","print(baselineLSTM.summary())"]},{"cell_type":"markdown","metadata":{},"source":["Train LSTM Model"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["\" \\nearly = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5) \\n#print(x_val.shape) \\nprint(x_train.shape) \\nprint(y_val_2.shape) \\nprint(y_train_2.shape) \\nbaselineLSTM.fit(x_train, y_train_2, epochs = 50, batch_size = 128, validation_data=(x_val, y_val_2), verbose=1, callbacks = [early]) \\n#INSERT PATH HERE TO SAVE MODEL TO \\npath = 'baseline_lstm.h5' \\nbaselineLSTM.save(path) \\n#Load Model \\nsavedBaselineLSTM = load_model(path) \\n\""]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["''' \n","early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5) \n","#print(x_val.shape) \n","print(x_train.shape) \n","print(y_val_2.shape) \n","print(y_train_2.shape) \n","baselineLSTM.fit(x_train, y_train_2, epochs = 50, batch_size = 128, validation_data=(x_val, y_val_2), verbose=1, callbacks = [early]) \n","#INSERT PATH HERE TO SAVE MODEL TO \n","path = 'baseline_lstm.h5' \n","baselineLSTM.save(path) \n","#Load Model \n","savedBaselineLSTM = load_model(path) \n","''' \n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["\"\\nhybrid = Sequential() \\nhybrid.add(LSTM(units = 16, return_sequences= True, input_shape = (x_train.shape[1:]))) \\nhybrid.add(Dropout(0.2)) \\nhybrid.add(GRU(units = 32, return_sequences=True)) \\nhybrid.add(Dropout(0.2)) \\nhybrid.add(LSTM(units = 32)) \\nhybrid.add(Dropout(0.2)) \\nhybrid.add(Dense(units = 43, activation='softmax')) \\nhybrid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) \\nprint(hybrid.summary()) \\nearly = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5) \\nhybrid.fit(x_train, y_train_2, epochs = 1, batch_size = 128, validation_data=(x_val, y_val_2), callbacks = [early]) \\npath = 'hybrid.h5' \\nhybrid.save(path) \\n\""]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","hybrid = Sequential() \n","hybrid.add(LSTM(units = 16, return_sequences= True, input_shape = (x_train.shape[1:]))) \n","hybrid.add(Dropout(0.2)) \n","hybrid.add(GRU(units = 32, return_sequences=True)) \n","hybrid.add(Dropout(0.2)) \n","hybrid.add(LSTM(units = 32)) \n","hybrid.add(Dropout(0.2)) \n","hybrid.add(Dense(units = 43, activation='softmax')) \n","hybrid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) \n","print(hybrid.summary()) \n","early = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5) \n","hybrid.fit(x_train, y_train_2, epochs = 1, batch_size = 128, validation_data=(x_val, y_val_2), callbacks = [early]) \n","path = 'hybrid.h5' \n","hybrid.save(path) \n","''' \n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["' \\nclass Model(HyperModel): \\n  def build(self, hp): \\n    units1 = hp.Int(\\'units1\\', min_value=32, max_value=512, step=32) \\n    dropout1 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \\n    units2 = hp.Int(\\'units2\\', min_value=32, max_value=512, step=32) \\n    dropout2 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \\n    units3 = hp.Int(\\'units3\\', min_value=32, max_value=512, step=32) \\n    dropout3 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \\n    units4 = hp.Int(\\'units2\\', min_value=32, max_value=512, step=32) \\n    dropout4 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \\n    model = Sequential() \\n    model.add(LSTM(units = units1, return_sequences= True, input_shape = (x_train.shape[1:]))) \\n    model.add(Dropout(dropout1)) \\n    model.add(LSTM(units = units2, return_sequences=True)) \\n    model.add(Dropout(dropout2)) \\n    model.add(LSTM(units = units3, return_sequences = True)) \\n    model.add(Dropout(dropout3)) \\n    model.add(LSTM(units = units4)) \\n    model.add(Dropout(dropout4)) \\n    model.add(Dense(units=43, activation = \\'softmax\\')) \\n    model.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\']) \\n    return model \\nhypermodel = Model() \\ntuner = keras_tuner.BayesianOptimization( \\n                        hypermodel=hypermodel, \\n                        objective = \"val_accuracy\", \\n                        max_trials =3, \\n                        overwrite=True, \\n                        directory=\\'BO_search_dir\\', \\n                        project_name=\\'better_lstm\\') \\ntuner.search(x_train, y_train_2, epochs=3, validation_data=(x_val, y_val_2)) \\nbest_model = tuner.get_best_models()[0] \\nbest_model.build(input_shape= (x_train.shape[1:])) \\nbest_model.summary() \\nprint(tuner.results_summary()) \\n'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["''' \n","class Model(HyperModel): \n","  def build(self, hp): \n","    units1 = hp.Int('units1', min_value=32, max_value=512, step=32) \n","    dropout1 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \n","    units2 = hp.Int('units2', min_value=32, max_value=512, step=32) \n","    dropout2 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \n","    units3 = hp.Int('units3', min_value=32, max_value=512, step=32) \n","    dropout3 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \n","    units4 = hp.Int('units2', min_value=32, max_value=512, step=32) \n","    dropout4 = hp.Float(name=\"dropout\", min_value=0.0, max_value=0.3, step=0.05) \n","    model = Sequential() \n","    model.add(LSTM(units = units1, return_sequences= True, input_shape = (x_train.shape[1:]))) \n","    model.add(Dropout(dropout1)) \n","    model.add(LSTM(units = units2, return_sequences=True)) \n","    model.add(Dropout(dropout2)) \n","    model.add(LSTM(units = units3, return_sequences = True)) \n","    model.add(Dropout(dropout3)) \n","    model.add(LSTM(units = units4)) \n","    model.add(Dropout(dropout4)) \n","    model.add(Dense(units=43, activation = 'softmax')) \n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) \n","    return model \n","hypermodel = Model() \n","tuner = keras_tuner.BayesianOptimization( \n","                        hypermodel=hypermodel, \n","                        objective = \"val_accuracy\", \n","                        max_trials =3, \n","                        overwrite=True, \n","                        directory='BO_search_dir', \n","                        project_name='better_lstm') \n","tuner.search(x_train, y_train_2, epochs=3, validation_data=(x_val, y_val_2)) \n","best_model = tuner.get_best_models()[0] \n","best_model.build(input_shape= (x_train.shape[1:])) \n","best_model.summary() \n","print(tuner.results_summary()) \n","''' \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":2}
